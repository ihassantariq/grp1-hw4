{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Find the duplicates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For answering this question we have downloaded text [**passwords2.txt**](https://drive.google.com/open?id=1wTmOU-yqk4qdQYg42AquhzgpNGrRA96d). Then to answer the first part of the question we have to define a hash function to answer this questions. **Find duplicate strings**\n",
    "\n",
    "But for the first part of this question we have to follow the definition of duplicates where order does not matter e.g  \"AABA\" = \"AAAB\" and \"AAB\"!= ABB\n",
    "\n",
    "There are three steps we have follow. \n",
    "1. Convert the string containing the password to a (potentially large) number\n",
    "2. Use a hash function to map the number to a large range. Read the class material and search the internet for this part (but you need to write the code yourself).\n",
    "3. After having defined the hash function, find if two numbers fall on the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # imported pandas\n",
    "import gmpy2\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chunk size 9999999 lines\n",
    "c_size = 9999999\n",
    "txt_file = \"passwords2.txt\" #text file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple comparison without considering order we really have to define very simple function as simple as it should be possible otherwise we will not have result as wanted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_hash_function(passw, table_size):\n",
    "    number = 0\n",
    "    for character in passw: # getting each character of the password\n",
    "        number += ord(character) # getting aschi of the character\n",
    "    return number % table_size # returns the modular result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_size_to_pass = 11 # should be prime for optimal result\n",
    "first_string = \"AABA\" \n",
    "second_string =\"AAAB\"\n",
    "first_value = simple_hash_function(first_string, table_size_to_pass)\n",
    "second_value = simple_hash_function(second_string, table_size_to_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[first_value,second_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above it *first_value* and *second_value* are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the file there are 10M duplicates, can you detect them?\n",
    "\n",
    "To asnwer this question I have to read the file which is 2.2G which will take so much memory and of - course we can detect them but right now there is very high chance for getting collision we will resolve this problem in second part of the question where we consider the order as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file_return_passwords(file_name, number_of_lines = 10000, starting_index = 0, consider_size = True):\n",
    "    i = 0\n",
    "    p_items = []\n",
    "    if(starting_index > 0):\n",
    "        number_of_lines +=  starting_index\n",
    "    c_size = number_of_lines\n",
    "    for line in open(txt_file):\n",
    "        if i == c_size and consider_size:\n",
    "            break\n",
    "        password = line.strip()\n",
    "        if not starting_index > i:\n",
    "            p_items.append(password)\n",
    "        i += 1\n",
    "    return p_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to read the file, lets move to answering the question detecting duplicate for this we need the Hash Table which size should be prime number for resolving performance issue. For the actual size as I am converting aschi characters to of 20 length passowrd. Each aschi-character length should not be greater than 127 as all the characters number is below this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_size = ord('z') *20*31   # considering we have 20 character and I have multiplied it prim number so that \n",
    "table_size = sympy.nextprime(table_size) # getting next prim number from sympy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table =[] # we will use chaining technique for hash table values\n",
    "passwords = open_file_return_passwords(txt_file, c_size)# open_file_and_return_passwords(txt_file, c_size)\n",
    "\n",
    "for i in range(table_size):\n",
    "    hash_table.append({}) # initializing table items\n",
    "i = 0\n",
    "\n",
    "for password in passwords:\n",
    "    table_index = simple_hash_function(password, table_size)\n",
    "    password = ''.join(sorted(password)) ## ignoring order of the password\n",
    "    if password not in hash_table[table_index].keys():\n",
    "        hash_table[table_index][password] = 0\n",
    "    hash_table[table_index][password] = hash_table[table_index][password] + 1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running above code we have everything we need to answer the questions but ofcourse we are only considering first **9999999** lines of *passwords2.txt* file. Lets find duplicates first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of duplicates \n",
    "duplicates = 0 # this duplicate consider that we have total duplicate item adding both number of duplicates \n",
    "# which can be more than 1 \n",
    "for item in hash_table:\n",
    "    i = 0\n",
    "    for key, value in item.items(): # dictionary item should have only one item otherwise they are false positive\n",
    "        if(value > 1):\n",
    "            duplicates += (value - 1) # removing 1 if only one count then that is not duplicate\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates in above **9999999** lines of the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Does it happen that two strings with different characters are hashed to the same value? If yes, could you provide the number of False Positive?\n",
    "\n",
    "Yes, it depends on size of the hash table. If size is less than number of passwords or anything that we are checking than there must be false positive. Lets find out how many false positive we have in our **9999999** of hash table size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75641"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we have **75641** size of the table and we are using chaining technique so we can detect that by checking how many items are false positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999063"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of duplicates \n",
    "false_positive = 0 # this duplicate consider that we have total duplicate item adding both number of duplicates \n",
    "# which can be more than 1 \n",
    "for item in hash_table:\n",
    "    i = 0\n",
    "    if (len(item.items()) > 1):\n",
    "        false_positive += (len(item.items()) - 1)  # removing 1 if only one count then that is not duplicate\n",
    "false_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so many false positive values which says that our hash function sucks. :P Now the total false postive values are 9999063 which says that our function is almost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_size-false_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only **936** times correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets move to define hash function. Here we are going to use the Multiplicative hash function. The two functions that works best and not too much complex are Multiplicative hash functions. Bernstein's function Kernighan and Ritchie's function. In which **Bernstein's function** uses *INITIAL_VALUE* of *5381* and *M* of *33*; **Kernighan and Ritchie's** function uses *INITIAL_VALUE* of 0 and *M* of 31. Here I am going to choose Bernstein's function. I am going to follow the **Bernstein's function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function_with_weighting(password, table_size, INITIAL_VALUE = 5381, M = 33 ): \n",
    "    hash_value = INITIAL_VALUE;\n",
    "    m = M\n",
    "    for i in range(len(password)):\n",
    "        hash_value =  m * hash_value + i * ord(password[i])\n",
    "    return hash_value % table_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both *INITIAL_VALUE* and *M* is prime number. And we will consider the prime number as well for considering our *Hash Table* size or *array* size. \n",
    "\n",
    "Lets answer the first question that **find if two numbers fall on the same range*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_size_to_pass = 11 # should be prime for optimal result\n",
    "first_string = \"AABA\" \n",
    "second_string =\"AAAB\"\n",
    "first_value = hash_function_with_weighting(first_string, table_size_to_pass)\n",
    "second_value = hash_function_with_weighting(second_string, table_size_to_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[first_value,second_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that obove both strings are not equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the file there are 10M duplicates, can you detect them?\n",
    "\n",
    "To answer this question now we also considering order now lets whether it will resolve issues with which we have so many false positives. We will also consider the same size of hash table. Lets see.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table =[] # we will use chaining technique for hash table values\n",
    "starting_index=99999999\n",
    "passwords = open_file_return_passwords(txt_file, c_size,starting_index)# open_file_and_return_passwords(txt_file, c_size)\n",
    "\n",
    "for i in range(table_size):\n",
    "    hash_table.append({}) # initializing table items\n",
    "i = 0\n",
    "\n",
    "for password in passwords:\n",
    "    table_index = hash_function_with_weighting(password, table_size)\n",
    "    if password not in hash_table[table_index].keys():\n",
    "        hash_table[table_index][password] = 0\n",
    "    hash_table[table_index][password] = hash_table[table_index][password] + 1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"8SyBU',YDgw*.ZAC#j4P\", 'OHcv-/U3QI$rdqYTef\"D', 'QtA*.xM$e(+\"aO36r&Uo']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passwords[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running above code we have everything we need to answer the questions but ofcourse we are only considering first **9999999** lines of *passwords2.txt* file. Lets find duplicates first. But duplicates in first 99999999 lines is never going to be appear as it was before so I have sent starting index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of duplicates \n",
    "duplicates = 0 # this duplicate consider that we have total duplicate item adding both number of duplicates \n",
    "# which can be more than 1 \n",
    "for item in hash_table:\n",
    "    i = 0\n",
    "    for key, value in item.items(): # dictionary item should have only one item otherwise they are false positive\n",
    "        if(value > 1):\n",
    "            duplicates += (value - 1) # removing 1 if only one count then that is not duplicate\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no duplicates in next **99999999** values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Does it happen that two strings with different characters are hashed to the same value? If yes, could you provide the number of False Positive?\n",
    "\n",
    "Yes, it depends on size of the hash table. If size is less than number of passwords or anything that we are checking than there must be false positive. Lets find out how many false positive we have in our **9999999** of hash table size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75641"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we have **75641** size of the table and we are using chaining technique so we can detect that by checking how many items are false positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9924358"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of duplicates \n",
    "false_positive = 0 # this duplicate consider that we have total duplicate item adding both number of duplicates \n",
    "# which can be more than 1 \n",
    "for item in hash_table:\n",
    "    i = 0\n",
    "    if (len(item.items()) > 1):\n",
    "        false_positive += (len(item.items()) - 1)  # removing 1 if only one count then that is not duplicate\n",
    "false_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that false positives are drastically decreased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75641"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_size-false_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our function is **75641** times correct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hash function is perfect enough to lower the false positive. So the best approach for the hashing is to use different hash functions randomly to decreased the chance of the false positives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
