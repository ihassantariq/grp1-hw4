{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Group 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to verify if the basic house informations reflect the house description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we imported all the libraries needed for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebaro\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:86: RuntimeWarning: The _imaging extension was built for another version of Pillow or PIL:\n",
      "Core version: 5.3.0\n",
      "Pillow version: 5.2.0\n",
      "  warnings.warn(str(v), RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The _imaging extension was built for another version of Pillow or PIL:\nCore version: 5.3.0\nPillow version: 5.2.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-aecc2c5b4820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m                           \u001b[1;34m\"Pillow version: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                           (getattr(core, 'PILLOW_VERSION', None),\n\u001b[1;32m---> 71\u001b[1;33m                            __version__))\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: The _imaging extension was built for another version of Pillow or PIL:\nCore version: 5.3.0\nPillow version: 5.2.0"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import threading\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part we perfomed the web scrapping from the web page www.immobiliare.it.\n",
    "\n",
    "We defined a function to get all the information needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scrapping(web_link):\n",
    "    \n",
    "    source = requests.get(web_link).text\n",
    "    soup = BeautifulSoup (source, 'lxml')\n",
    "    info_matrix = []\n",
    "    \n",
    "    for announcement in soup.find_all('div', class_ = 'listing-item_body'):\n",
    "        try:\n",
    "            title = announcement.a.text\n",
    "            price = announcement.li.text\n",
    "        \n",
    "            v=[]\n",
    "            i=0\n",
    "        \n",
    "            for dati in announcement.find_all('div', class_ = 'lif__data'):\n",
    "                a = dati.text\n",
    "                v.append(a)\n",
    "                i = i+1\n",
    "            \n",
    "            locali = v[0]\n",
    "            superficie = v[1]\n",
    "            bagni = v[2]\n",
    "            piano = v[3]\n",
    "        \n",
    "            link =announcement.a['href']\n",
    "            if link.find('https://www.immobiliare.it') == -1:\n",
    "                link = 'https://www.immobiliare.it' + link\n",
    "         \n",
    "            source_desc = requests.get(link).text\n",
    "            soup1 = BeautifulSoup (source_desc, 'lxml')\n",
    "            description = soup1.find('div', class_ = 'description-text').text\n",
    "       \n",
    "            announce = [title, price, locali, superficie, bagni, piano, link, description]\n",
    "        \n",
    "            info_matrix.append(announce)\n",
    "        except:\n",
    "            continue\n",
    "          \n",
    "    return info_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = page_scrapping('https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=1')\n",
    "matrix = np.array([np.array(xi) for xi in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 600):\n",
    "    try:\n",
    "        A = page_scrapping('https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=' + str(i))\n",
    "        newMatrix = np.array([np.array(xi) for xi in A])\n",
    "        print('Download' + str(i))\n",
    "        matrix = np.concatenate((matrix, newMatrix), axis = 0)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the data, we created a dataframe for them. And we save it in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(matrix)\n",
    "df.columns = ['Title', 'Price', 'Locali', 'Superficie', 'Bagni', 'Piano', 'Link', 'Description']\n",
    "df.to_pickle('data_immobiliare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data_immobiliare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our dataframe, we need to clean our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we cleaned the informations as 'Price', 'Piano', 'Locali' in order to have all float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.Price.loc[i] = re.sub('[€]', '' , df.Price.loc[i])\n",
    "    df.Price.loc[i] = df.Price.loc[i].replace(\"PREZZO SU RICHIESTA\", \"0\")\n",
    "    df.Price.loc[i] = df['Price'].loc[i].split()[0]\n",
    "    df.Price.loc[i] = df.Price.loc[i].replace(\".\", \"\")\n",
    "    \n",
    "    df.Piano.loc[i] = re.sub('T', '0', df.Piano.loc[i])\n",
    "    df.Piano.loc[i] = re.sub('R', '0.5', df.Piano.loc[i])\n",
    "    df.Piano.loc[i] = re.sub('A', '100', df.Piano.loc[i])\n",
    "    df.Piano.loc[i] = re.sub('S', '-1', df.Piano.loc[i])\n",
    "    df.Piano.loc[i] = re.sub(' \\n', '', df.Piano.loc[i])\n",
    "    df.Piano.loc[i] = df.Piano.loc[i].replace(\"11+\", \"11.5\")\n",
    "    \n",
    "    \n",
    "    df[\"Bagni\"][i] = df[\"Bagni\"][i].replace(\"+\", \".5\")\n",
    "    \n",
    "    df[\"Locali\"][i] = df[\"Locali\"][i].replace(\"+\", \".5\")\n",
    "    \n",
    "    df.replace(df.Superficie.loc[i], df.Superficie.loc[i].split()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.replace(df.Price.loc[i], float(df.Price.loc[i]), inplace = True)\n",
    "        df.replace(df.Locali.loc[i], float(df.Locali.loc[i]), inplace = True)\n",
    "        df.replace(df.Superficie.loc[i], float(df.Superficie.loc[i]), inplace = True)\n",
    "        df.replace(df.Bagni.loc[i], float(df.Bagni.loc[i]), inplace = True)\n",
    "        df.replace(df.Piano.loc[i], float(df.Piano.loc[i]), inplace = True)\n",
    "        \n",
    "    except:\n",
    "        print('errore ' + str(i))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create two matrixes: one for the informations, and one for the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = df[['Price', 'Locali', 'Superficie', 'Bagni', 'Piano']].copy()\n",
    "df_desc = df.filter(['Description'], axis=1)\n",
    "\n",
    "df_info.to_pickle('df_info')\n",
    "df_desc.to_pickle('df_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to work on the description matrix: it's defined here a data cleaning function, used to clean all the reviews' texts and later on the queries.\n",
    "A number of steps are performed for the task:\n",
    "- replace $ with 'dollar', to avoid losing relevant information from the text\n",
    "- replace new line symbols with whitespace\n",
    "- remove puntuation\n",
    "- separate numbers from words\n",
    "- remove stopwords\n",
    "- tokenize the text\n",
    "- stem of the words, just keeping the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleaning(string_raw):\n",
    "    m = string_raw\n",
    "    #removing website url\n",
    "    m = re.sub('http.* *', '', string_raw, flags=re.MULTILINE)\n",
    "    m = re.sub(r'\\d+', '', string_raw)\n",
    "    m = m.replace('\\\\r', ' ').replace('\\\\n', ' ')\n",
    "    m = re.sub('[%s]' % re.escape(string.punctuation), ' ', m)\n",
    "    m = re.sub('[%s]' % re.escape('“”€'), ' ', m)\n",
    "    m = re.sub('[%s]' % re.escape('”'), ' ', m)\n",
    "    m = re.sub(r'(?<=[\\d+])(?=[a-zA-Z_])', r' ', m)\n",
    "    m = nltk.tokenize.word_tokenize(m)\n",
    "    m = [word for word in m if word.lower() not in stopwords.words('italian')]\n",
    "    sno = nltk.stem.SnowballStemmer('italian')\n",
    "    string_new = [sno.stem(word) for word in m]\n",
    "    \n",
    "    return string_new\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a full corpus with all the words appeared in the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(df_desc)):\n",
    "    corpus.append(df_desc.iloc[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the corpus it's performed the data cleaning and tokenization and the corpus has been flattened for the words' occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "for i in range(len(corpus)):\n",
    "    lista.append(string_cleaning(corpus[i]))\n",
    "    \n",
    "\n",
    "lista_flatten = [y for x in lista for y in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a dictionary with all the words counted and then sorted by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(lista_flatten)\n",
    "c = dict(c.most_common())\n",
    "\n",
    "final_dict = {str(i+1):x for i,x in enumerate(c)}\n",
    "final_dict_inv = {str(x):(i+1) for i,x in enumerate(c)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final_dict we assigned a number to each word, and the reverse in the final_dict_inv, that we are using to build the cosine similarity index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = {}\n",
    "for n,document in enumerate(lista):\n",
    "    new_document = []\n",
    "    for word in document:\n",
    "        new_document.append(final_dict_inv[str(word)])\n",
    "    indx[str(n+1)] = new_document \n",
    "    \n",
    "inv_indx = {str(i):[] for i in list(final_dict.keys())}\n",
    "for word in list(final_dict.keys()):\n",
    "    for i in range(len(indx)):\n",
    "        if int(word) in indx[str(i+1)]:\n",
    "            inv_indx[word].append(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the indx dictionary for each announce we reported the list of words that appear in it whit the correspondent number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Cosine similarity it's needed to calcolate for each term:\n",
    "- the frequency in each document: TF\n",
    "- the IDF$_{i}$ defined has: $$IDF_{i}=\\log{\\frac{n}{N_{i}}}$$\n",
    "where i is refered to term i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count= len(df_desc)\n",
    "\n",
    "inv_indx_cosine = {i:[] for i in list(final_dict.keys())}\n",
    "for word in list(final_dict.keys()):\n",
    "    for i,l in enumerate(list(indx.values())):\n",
    "        if int(word) in l:\n",
    "            tf = l.count(int(word))/len(l)\n",
    "            id_f = log(file_count/len(inv_indx[word]))\n",
    "            inv_indx_cosine[word].append((i+1,tf*id_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = len(indx) + 2\n",
    "cols = len(inv_indx)\n",
    "matrix = np.zeros((raws, cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able now to build a matrix with the announments on the raws and the words on the columns, were are recorded the tf-idf indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indx)):\n",
    "    for j in range(len(a[i])):\n",
    "        col = int (a[i][j][0])\n",
    "        print('i ' + str(i))\n",
    "        print(j)\n",
    "        print(col)\n",
    "        \n",
    "        matrix[i][col] = a[i][j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are saving that dataframe so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.to_pickle('df_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.read_pickle(\"df_tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the K-means++ clustering to our info matrix first and than to the tf-idf matrix, related to the description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_info = KMeans(n_clusters = 3).fit(df_info)\n",
    "\n",
    "cls_info = dict()\n",
    "\n",
    "for i in range(1, len(kmeans_info.labels_)):\n",
    "    if kmeans_info.labels_[i] not in cls_info:\n",
    "        cls_info[kmeans_info.labels_[i]] = [i]\n",
    "    elif kmeans_info.labels_[i] in cls_info:\n",
    "        cls_info[kmeans_info.labels_[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_desc = KMeans(n_clusters = 6).fit(df_tfidf)\n",
    "\n",
    "cls_desc = dict()\n",
    "\n",
    "for i in range(1, len(kmeans_desc.labels_)):\n",
    "    if kmeans_desc.labels_[i] not in cls_desc:\n",
    "        cls_desc[kmeans_desc.labels_[i]] = [i]\n",
    "    elif kmeans_desc.labels_[i] in cls_desc:\n",
    "        cls_desc[kmeans_desc.labels_[i]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a, b):\n",
    "   c = set(a).intersection(b)\n",
    "   return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[]\n",
    "for i in range(6):\n",
    "    v = []\n",
    "    for j in range(3):\n",
    "        jac = jaccard(cls_desc.get(i), cls_info.get(j))\n",
    "        v.append(jac)\n",
    "    A.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.017250284061563887, 0.002857142857142857, 0.016683518705763397],\n",
       " [0.07667698658410732, 0.0021413276231263384, 0.01604696673189824],\n",
       " [0.01695440918019229, 0.0, 0.013171225937183385],\n",
       " [0.5968183826778612, 0.01660538488909975, 0.17530155083285467],\n",
       " [0.17742755465175394, 0.002888781896966779, 0.05110923897781522],\n",
       " [0.007772020725388601, 0.004405286343612335, 0.0010604453870625664]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_couple_clustor = (cls_desc.get(3), cls_info.get(0))\n",
    "second_couple_clustor = (cls_desc.get(3), cls_info.get(2))\n",
    "third_couple_clustor = (cls_desc.get(4), cls_info.get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_clustors = [first_couple_clustor,second_couple_clustor,third_couple_clustor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then extract the descrpition from the clustors using description datframe above and will create wordcloud.\n",
    "\n",
    "We have to mask wordcloud to the image that I have downloaded as house. For this I will create a matrix mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-7545a0617c56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhouse_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"house.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhouse_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse_mask\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "house_mask = np.array(Image.open(\"house.png\"))\n",
    "house_mask[house_mask > 0] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
